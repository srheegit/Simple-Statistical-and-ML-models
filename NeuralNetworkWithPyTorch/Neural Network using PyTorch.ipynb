{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b9ec89-bcbe-4751-a64e-18a84696226c",
   "metadata": {},
   "source": [
    "# Neural Network Using PyTorch\n",
    "\n",
    "Author: Sahngyoon Rhee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bd950-39b5-4a06-b988-dc32796d9f51",
   "metadata": {},
   "source": [
    "Neural Network puts together various simplier machine learning models, increasing the complexity and therefore flexibility in the final model. Its idea is derived from the way that some neuroscientists think that the nuerons in the human brain works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0250e1c2-9429-4e06-897c-a04b0427f28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style = \"display: flex; justify-content: space-around;\">\n",
       "    <img src='human_neurons.jpg' width='500' height='300'>\n",
       "    <img src='neural_network.jpg' width='400' height='300'>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style = \"display: flex; justify-content: space-around;\">\n",
    "    <img src='human_neurons.jpg' width='500' height='300'>\n",
    "    <img src='neural_network.jpg' width='400' height='300'>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99955bc-4afa-4ca1-b6fa-06c8967b3ff7",
   "metadata": {},
   "source": [
    "The goal of this notebook is not to explain the nitty gritty details of how a neural network works. That would involve explaining the forward propagation, backward propagation, gradient descent, activation functions, hidden layers, etc. and you can find the information easily online.\n",
    "\n",
    "The goal of this notebook is to demonstrate how to build a simple neural network using PyTorch, which along with Tensorflow is one of the most popular deep learning frameworks in Python.\n",
    "\n",
    "The following code is from PyTorch's starter guide, and it uses the `FashionMNIST` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c46fb3-15f1-4309-8527-5f92be70ca9b",
   "metadata": {},
   "source": [
    "## Preparing data for training\n",
    "\n",
    "We first import the necessary packages and our data. We then use the `Dataset` under `torch.utils.data` - `Dataset` retrives our dataset's features and labels one sample at a time. Later, we shall use `DataLoader` to wrap the `Dataset` as iterable and in batches. We shall later pass sample in minibatches, reshuffling the data at every epoch to reduce model overfitting, and us Python's `multiprocessing` to retrieve our data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e950d30-7d5c-4bda-928e-886a45af17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line if you don't have PyTorch installed yet\n",
    "# it'll take a couple minutes to finish install\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a1789a-31ec-455e-b974-a558838a5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data', # path where the train/test data is stored\n",
    "    train=True, # if False, it means that the dataset is test dataset\n",
    "    download=True, # downloads the data from the internet if it's not available at `root`\n",
    "    transform=ToTensor() # specify the feature transformations\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f4fb3-273f-4c24-bdd5-2bb3b29d7a09",
   "metadata": {},
   "source": [
    "Now that we have loaded our data, we shall wrap them as iterables using `DataLoader`. Notice that the `s` in `torch.utils.data.Dataset` is in small case while the `L` in `DataLoader` is capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397f5dca-1108-437f-a180-a1de256f9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ba79d-a978-429a-8974-8822da77da28",
   "metadata": {},
   "source": [
    "We can now iterate through the DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69152322-c77b-42bf-a61d-9b1745a7a4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We shall visualize a sample image. Notice that the batches reshuffle afterall batches have been iterated through. \n",
      "\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGxCAYAAADLfglZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoO0lEQVR4nO3dfXRU5YHH8d+QlyGEIRIhmYSXNBbYpg0iAvKi8tYSCZUjBC1Kq0A9HrsCHjZ6qpE9S2pbYllh21Na9rTr8rJK4WgpvmDFQEiwAt1IcaGIbCyhhMoUCZAJkbzf/YPDrGMC5rkkefLy/Zxzz2Hu3F/uMzc3+XEzM894HMdxBACABT1sDwAA0H1RQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQrBq/fr18ng8oSUyMlJJSUm6//77VVJSYm1cubm58ng81vb/eYWFhfJ4PHrllVdsDwVoVZG2BwBI0rp16/SVr3xF1dXVevfdd/XjH/9Yu3fv1ocffqi+ffvaHh6ANkIJoUNIT0/X6NGjJUmTJ09WQ0ODli9frm3btmnhwoWWRwegrfDnOHRIVwrp73//e2hddXW1nnjiCd1yyy2Ki4tTfHy8xo8fr1dffbVJ3uPxaPHixfqv//ovpaWlqVevXhoxYoTeeOONJttu375dt9xyi7xer1JTU/X88883O6bq6mrl5OQoNTVV0dHRGjBggBYtWqQLFy6EbfelL31Jd999t9544w2NHDlSMTExSktLC+17/fr1SktLU2xsrG677Ta99957ro7RlT8ZHjp0SPfdd1/omGRnZ6u+vl7Hjh3T9OnT5fP59KUvfUkrV65s8nhaejwvXLighx9+WPHx8erdu7e++c1v6vjx4/J4PMrNzQ3btqSkRPPmzVNCQoK8Xq/S0tL0i1/8wtVjRNfHlRA6pNLSUknSsGHDQutqamp07tw5PfnkkxowYIBqa2u1c+dOZWVlad26dXrooYfCvsb27dtVXFysZ599Vr1799bKlSs1e/ZsHTt2TDfddJMkadeuXbrnnns0fvx4bd68WQ0NDVq5cmVY+UmS4ziaNWuWdu3apZycHN155506dOiQli9frn379mnfvn3yer2h7f/nf/5HOTk5WrZsmeLi4vSDH/xAWVlZysnJ0a5du7RixQp5PB499dRTuvvuu1VaWqqYmBhXx+pb3/qWvvOd7+jRRx9Vfn6+Vq5cqbq6Ou3cuVOPPfaYnnzySW3atElPPfWUhgwZoqysLKPj2djYqJkzZ+q9995Tbm6ubr31Vu3bt0/Tp09vMpYPPvhAEyZM0ODBg7Vq1Sr5/X7t2LFDjz/+uM6ePavly5e7eozowhzAonXr1jmSnP379zt1dXVOZWWl89Zbbzl+v9+ZOHGiU1dXd9VsfX29U1dX5zz88MPOyJEjw+6T5CQmJjrBYDC0LhAIOD169HDy8vJC68aOHeskJyc7ly5dCq0LBoNOfHy889kfj7feesuR5KxcuTJsP1u2bHEkOb/61a9C61JSUpyYmBjn1KlToXXvv/++I8lJSkpyqqqqQuu3bdvmSHJee+21ax6n3bt3O5Kcl19+ObRu+fLljiRn1apVYdvecsstjiRn69atoXV1dXVO//79naysrKvu42rHc/v27Y4kZ+3atWHb5+XlOZKc5cuXh9bdddddzsCBA52KioqwbRcvXuz07NnTOXfu3DUfJ7of/hyHDmHcuHGKioqSz+fT9OnT1bdvX7366quKjAy/WH/55Zd1++23q3fv3oqMjFRUVJReeOEFHT16tMnXnDJlinw+X+h2YmKiEhIS9Ne//lWSVFVVpeLiYmVlZalnz56h7Xw+n2bOnBn2tQoKCiRJCxYsCFt/3333KTY2Vrt27Qpbf8stt2jAgAGh22lpaZIuP9/Vq1evJuuvjMmNu+++O+x2WlqaPB6PMjMzQ+siIyM1ZMiQJvtpyfEsKiqSdPmK67MeeOCBsNvV1dXatWuXZs+erV69eqm+vj60zJgxQ9XV1dq/f7/rx4muiRJCh7Bx40YVFxeroKBAjz76qI4ePdrkl9zWrVv1rW99SwMGDNCLL76offv2qbi4WN/97ndVXV3d5GveeOONTdZ5vV5dunRJknT+/Hk1NjbK7/c32e7z68rLyxUZGan+/fuHrfd4PPL7/SovLw9bHx8fH3Y7Ojr6muubG39LNfc1e/XqFVasV9Z/dj8tPZ5XHvvn95OYmBh2u7y8XPX19fr5z3+uqKiosGXGjBmSpLNnz7p+nOiaeE4IHUJaWlroxQhTpkxRQ0OD/uM//kOvvPKK7r33XknSiy++qNTUVG3ZsiXsPTw1NTWu9tm3b195PB4FAoEm931+3Y033qj6+np98sknYUXkOI4CgYDGjBnjagw2tfR4Xnns586dCyuizx+jvn37KiIiQg8++KAWLVrU7D5TU1Nb8RGgK+BKCB3SypUr1bdvX/3Lv/yLGhsbJV2+6oiOjg77hRkIBJp9NVdLXHl12tatW8P+519ZWanXX389bNuvf/3rki7/4v6s3/72t6qqqgrd35m09HhOmjRJkrRly5aw9Zs3bw673atXL02ZMkUHDx7UzTffrNGjRzdZmrs6RfdGCaFD6tu3r3JycnT06FFt2rRJ0uXnPo4dO6bHHntMBQUF2rBhg+644w4lJSW53s8Pf/hDBQIBTZs2Tdu2bdNvf/tbff3rX1dsbGzYdtOmTdNdd92lp556Sj/4wQ+0c+dOrV69WgsXLtTIkSP14IMPXtfjtaGlx3P69Om6/fbb9cQTT+gnP/mJdu7cqR/+8Id64YUXJEk9evz/r5Gf/exnOnnypO68806tX79ehYWFev311/Vv//Zvmjp1ars+PnQOlBA6rCVLlmjw4MF69tln1dDQoIULF+q5557T73//e82YMUM/+clP9PTTT2vevHmu93GlfILBoObOnavs7GzNmTNH3/3ud8O283g82rZtm7Kzs7Vu3TrNmDFDzz//vB588EEVFBSEvTy7s2jp8ezRo4def/113X///Xruued0zz336J133gldFd5www2hbb/61a/qT3/6k9LT0/XP//zPysjI0MMPP6xXXnmlU14tou15HMdxbA8CQOezadMmffvb39a7776rCRMm2B4OOilKCMAX+s1vfqO//e1vGj58uHr06KH9+/frX//1XzVy5MjQS7gBN3h1HIAv5PP5tHnzZv3oRz9SVVWVkpKStGDBAv3oRz+yPTR0clwJAQCs4YUJAABrKCEAgDWUEADAmg73woTGxkZ9/PHH8vl8HerjlQEALeM4jiorK5WcnBz2ZubmdLgS+vjjjzVo0CDbwwAAXKeysjINHDjwmtt0uD/HfXbqfQBA59WS3+dtVkK//OUvlZqaqp49e2rUqFF65513WpTjT3AA0DW05Pd5m5TQli1btHTpUi1btkwHDx7UnXfeqczMTJ08ebItdgcA6KTa5M2qY8eO1a233qq1a9eG1qWlpWnWrFnKy8u7ZjYYDCouLq61hwQAaGcVFRXq06fPNbdp9Suh2tpaHThwQBkZGWHrMzIytHfv3ibb19TUKBgMhi0AgO6h1Uvo7NmzamhoaPLRv4mJic1+gmVeXp7i4uJCC6+MA4Duo81emPD5J6Qcx2n2SaqcnBxVVFSElrKysrYaEgCgg2n19wn169dPERERTa56zpw50+TqSJK8Xm+n/EAwAMD1a/UroejoaI0aNUr5+flh6/Pz8/ngKwBAmDaZMSE7O1sPPvigRo8erfHjx+tXv/qVTp48qe9973ttsTsAQCfVJiU0d+5clZeX69lnn9Xp06eVnp6uN998UykpKW2xOwBAJ9XhPtSO9wkBQNdg5X1CAAC0FCUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsaZNZtAG0Pjefx3Xy5ElX+zp16pSrHNyJjDT/VTxq1ChX+/rjH/9onGnuU7GvxWRebK6EAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYI3HMZnutB0Eg0HFxcXZHga6qYiICONMQ0NDG4ykKTc/qh999JGrfQ0dOtRVDu4MHjzYOPOzn/3M1b5mz55tnHE7i3ZFRYX69OlzzW25EgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayJtDwDoSNprPt85c+YYZ/785z8bZ06cOGGckaT//M//NM74fD7jzIULF4wzlZWVxhnTCTiviI+PN86UlJQYZ2666SbjzH//938bZ9xqy58LroQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBomMAU+o7GxsV32M2vWLONMbW2tcaahocE4I0kLFy40zpw8edI442ZiUTeTaW7cuNE4I0kbNmwwzgwbNsw4k5GRYZx59913jTNumX6fTL5HXAkBAKyhhAAA1rR6CeXm5srj8YQtfr+/tXcDAOgC2uQ5oa997WvauXNn6HZERERb7AYA0Mm1SQlFRkZy9QMA+EJt8pxQSUmJkpOTlZqaqvvvv1/Hjx+/6rY1NTUKBoNhCwCge2j1Eho7dqw2btyoHTt26Ne//rUCgYAmTJig8vLyZrfPy8tTXFxcaBk0aFBrDwkA0EG1egllZmZqzpw5Gj58uL7xjW9o+/btkq7+evucnBxVVFSElrKystYeEgCgg2rzN6vGxsZq+PDhKikpafZ+r9crr9fb1sMAAHRAbf4+oZqaGh09elRJSUltvSsAQCfT6iX05JNPqqioSKWlpfrjH/+oe++9V8FgUPPnz2/tXQEAOrlW/3PcqVOn9MADD+js2bPq37+/xo0bp/379yslJaW1dwUA6ORavYQ2b97c2l8SMOb2DdJuJ/w0ddtttxlnzp49a5yJiooyzkhSaWmpccbNi4rcjM/NBKH/+7//a5yRpL/85S/GmZtvvtk4Exsba5xpr8l2JXeTxrYUc8cBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDVt/qF2gA3tOYHpkCFDjDOpqanGmXfffdc4M3jwYOOMJHk8HuOMm0k4o6OjjTOXLl0yziQkJBhnJCkrK8s4k5ycbJxxMxnp0KFDjTMdEVdCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGANJQQAsIZZtNEl1dXVtdu+vvnNbxpn3MwE/cknnxhnBg0aZJyRpMhI818NbmaCdjNruRtu99OzZ0/jjOM4xplz584ZZ+655x7jjCQ988wzrnJthSshAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGCUzRJbmZRNKtOXPmGGdOnDhhnKmtrTXOuJmAU3I3AWyPHu3zf1o3E6W6PR/OnDljnPnyl79snFmzZo1x5uc//7lxRpImTpxonNmzZ4+rfbUEV0IAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0TmKLDi4iIMM40NDS42ldkpPmPRHp6unHmjTfeMM64mYzU6/UaZyR3E366mcDUTaa+vt4443YC00uXLrVLxs1j2rhxo3FGknJycowzTGAKAOiSKCEAgDXGJbRnzx7NnDlTycnJ8ng82rZtW9j9juMoNzdXycnJiomJ0eTJk3XkyJHWGi8AoAsxLqGqqiqNGDHiqh/CtHLlSq1evVpr1qxRcXGx/H6/pk2bpsrKyuseLACgazF+FjYzM1OZmZnN3uc4jn76059q2bJlysrKkiRt2LBBiYmJ2rRpkx599NHrGy0AoEtp1eeESktLFQgElJGREVrn9Xo1adIk7d27t9lMTU2NgsFg2AIA6B5atYQCgYAkKTExMWx9YmJi6L7Py8vLU1xcXGgZNGhQaw4JANCBtcmr4zweT9htx3GarLsiJydHFRUVoaWsrKwthgQA6IBa9c2qfr9f0uUroqSkpND6M2fONLk6usLr9bp+Qx0AoHNr1Suh1NRU+f1+5efnh9bV1taqqKhIEyZMaM1dAQC6AOMroYsXL+qjjz4K3S4tLdX777+v+Ph4DR48WEuXLtWKFSs0dOhQDR06VCtWrFCvXr00b968Vh04AKDzMy6h9957T1OmTAndzs7OliTNnz9f69ev1/e//31dunRJjz32mM6fP6+xY8fq7bffls/na71RAwC6BOMSmjx58jUnA/R4PMrNzVVubu71jAsIaWxsbLd93XvvvcYZNxOLlpSUGGf69OljnKmpqTHOSO4mjXUzSaibjJvJad0eh+joaONMVFSUceYf/uEfjDNvv/22cUaS/umf/sk4Y3oR4TiOLl682KJtmTsOAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1rTqJ6sCXyQy0vyUq6+vb4ORNO/KR5OY+Ozna7WUm1md3czOXFlZaZyR3M0e7Wa285iYGONMXV2dccbNeSe5m93azQzkbo6328d04MAB48zjjz9utH1NTY2ef/75Fm3LlRAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWMMEpmhXbiZqdDOB6Q033GCckaSbb77ZOLN+/XrjTL9+/YwzsbGx7ZKR3E2w6ub75GYSzgsXLhhn/H6/cUaS0tLSjDNer9c48/777xtnqqurjTOSVFhYaJwZNWqU0faffvppi7flSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArGECU7Qrk4kNr8eKFStc5aqqqowz58+fN8706dPHOBMIBIwzt956q3FGkhobG9sl42YC08rKSuNMRESEcUZy95g+/PBD48ypU6eMM27G5jZnOiFwVFRUi7flSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArGECU3R40dHRxpmFCxe62tebb75pnHEzvurqauNMRUWFcaZ3797GGcndRLM9epj/n9bN+E6ePGmcaWhoMM5IUmlpqXHG6/UaZ6ZNm2accfu9HTRokHHmyJEjRttfunSpxdtyJQQAsIYSAgBYY1xCe/bs0cyZM5WcnCyPx6Nt27aF3b9gwQJ5PJ6wZdy4ca01XgBAF2JcQlVVVRoxYoTWrFlz1W2mT5+u06dPhxY3f2cHAHR9xi9MyMzMVGZm5jW38Xq98vv9rgcFAOge2uQ5ocLCQiUkJGjYsGF65JFHdObMmatuW1NTo2AwGLYAALqHVi+hzMxMvfTSSyooKNCqVatUXFysqVOnqqamptnt8/LyFBcXF1rcvHwQANA5tfr7hObOnRv6d3p6ukaPHq2UlBRt375dWVlZTbbPyclRdnZ26HYwGKSIAKCbaPM3qyYlJSklJUUlJSXN3u/1el29uQsA0Pm1+fuEysvLVVZWpqSkpLbeFQCgkzG+Erp48aI++uij0O3S0lK9//77io+PV3x8vHJzczVnzhwlJSXpxIkTeuaZZ9SvXz/Nnj27VQcOAOj8jEvovffe05QpU0K3rzyfM3/+fK1du1aHDx/Wxo0bdeHCBSUlJWnKlCnasmWLfD5f640aANAlGJfQ5MmT5TjOVe/fsWPHdQ0InUdkpPlTivX19caZp59+2jhz7tw544wklZWVGWfcTEYaERFhnBkwYIBx5sYbbzTOSLrm2yquxs2f3N1MyupmUtG4uDjjjCQNHTrUOONmQtuEhATjjJvzQbr8gjFTzzzzjNH2tbW1Ld6WueMAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgTZt/smpH5vF4XOV69DDv7oaGhnbZj5vH5GZskrsZsfv162ecWbRokXHmT3/6k3FGcjcT9AcffGCcueGGG4wzI0eONM64/d66mU38/PnzxplAIGCcGTNmjHFm2rRpxhlJrj71+eLFi8aZmpoa44ybmc4ld79Xjh8/brR9XV1di7flSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArOmwE5h6PB6jyTjdTMrnZgJOyf2kkKYaGxvbZT9uRUREGGd2795tnDl16pRxpra21jgjSdXV1cYZN5OyutnP66+/bpzZtWuXcUaSUlNTjTNuJmWdN2+ecaZ///7GmYMHDxpnJHfj27t3r3HGzbE7d+6ccUaSIiPNf+2b/gya/I7kSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArOmwE5g6jiPHcVq8fUef7LO9DBkyxDjzne98x9W+HnroIePM+fPnjTNlZWXGGbfnw4cffmic6dWrl3EmOjraOHPjjTcaZ26//XbjjCT5/X7jjJuJXOPj440z9913n3HmlVdeMc60JzcTMJtM8PxZbib3/fvf/260vcnPH1dCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGBNh53AVDKboM/NBIVf/epXjTOSVFFRYZypq6szzriZRDI5Odk4c+HCBeOMJJWUlBhngsGgcaZnz57GmQMHDhhnJKm8vNw4k5iYaJxJS0szzriZ5DI2NtY4I0l9+/Y1zvz5z382zkyaNMk409FFRUUZZ+rr640zbiYildxN7mv6c2sy+TRXQgAAayghAIA1RiWUl5enMWPGyOfzKSEhQbNmzdKxY8fCtnEcR7m5uUpOTlZMTIwmT56sI0eOtOqgAQBdg1EJFRUVadGiRdq/f7/y8/NVX1+vjIwMVVVVhbZZuXKlVq9erTVr1qi4uFh+v1/Tpk1TZWVlqw8eANC5Gb0w4a233gq7vW7dOiUkJOjAgQOaOHGiHMfRT3/6Uy1btkxZWVmSpA0bNigxMVGbNm3So48+2nojBwB0etf1nNCVV4ld+Yje0tJSBQIBZWRkhLbxer2aNGmS9u7d2+zXqKmpUTAYDFsAAN2D6xJyHEfZ2dm64447lJ6eLkkKBAKSmr5kNTExMXTf5+Xl5SkuLi60DBo0yO2QAACdjOsSWrx4sQ4dOqTf/OY3Te77/Pt7HMe56nt+cnJyVFFREVrKysrcDgkA0Mm4erPqkiVL9Nprr2nPnj0aOHBgaP2VN1cGAgElJSWF1p85c+aqb+jzer3yer1uhgEA6OSMroQcx9HixYu1detWFRQUKDU1Nez+1NRU+f1+5efnh9bV1taqqKhIEyZMaJ0RAwC6DKMroUWLFmnTpk169dVX5fP5Qs/zxMXFKSYmRh6PR0uXLtWKFSs0dOhQDR06VCtWrFCvXr00b968NnkAAIDOy6iE1q5dK0maPHly2Pp169ZpwYIFkqTvf//7unTpkh577DGdP39eY8eO1dtvvy2fz9cqAwYAdB0ex2SmuXYQDAYVFxenb3zjG4qMbHlHvvrqq8b7+v3vf2+ckeTqOazevXsbZy5evGiccfMS95qaGuOMJDU0NBhnPvnkE+OMm4kxIyIijDOSNHXqVONMQkKCccbNBKFujt348eONM5L08ssvG2fa632Abr63bs5Vt/7yl78YZ2666SbjzMGDB40zkrsJga9nsuc+ffpccxvmjgMAWEMJAQCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1rj5ZtT3069dP0dHRLd7+yJEjxvs4evSocUaSysvLjTPV1dXGmat9JPq1uJlh2M3YJKm+vt44Exsba5xxM8Pw8OHDjTPS5fPOlJuZy48dO2acGTFihHHmpZdeMs5Ilz89uaNqzxmx3aioqGiX/bj5/SBd/nDSjoQrIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwpsNOYPrtb3/baLLLXr16Ge9j3LhxxhlJqqurM85cuHDBOONmYszz588bZ9w8HsndBKY+n884k5qaapyJiYkxzkjS3/72N+NMVVWVceauu+4yzrj53nbkiUi7qosXL7bLftxO5Hrp0qVWHsn14UoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKzpsBOYRkREKCIiosXbp6SkGO/DzaSnkhQZaX7Y3ExG6maCQpNjdoWbiTElqWfPnsaZxsZGV/sy5XZS1j59+rTySJo3aNAg48zYsWPbYCTN69HD/P+n7fW97ehOnTrVLvtxO4FpbW1tK4/k+nAlBACwhhICAFhDCQEArKGEAADWUEIAAGsoIQCANZQQAMAaSggAYA0lBACwhhICAFhDCQEArKGEAADWdNgJTGfMmGG0/aRJk4z38cADDxhnJHeTY86ZM8c4k5SUZJypr683zriZkLU9Xbx4sV0ybnNDhgwxzsydO9c4c/r0aeNMdHS0cUZyPwEspLS0tHbZj9vvrcfjaeWRXB+uhAAA1lBCAABrjEooLy9PY8aMkc/nU0JCgmbNmqVjx46FbbNgwQJ5PJ6wZdy4ca06aABA12BUQkVFRVq0aJH279+v/Px81dfXKyMjQ1VVVWHbTZ8+XadPnw4tb775ZqsOGgDQNRg9I/3WW2+F3V63bp0SEhJ04MABTZw4MbTe6/XK7/e3zggBAF3WdT0nVFFRIUmKj48PW19YWKiEhAQNGzZMjzzyiM6cOXPVr1FTU6NgMBi2AAC6B9cl5DiOsrOzdccddyg9PT20PjMzUy+99JIKCgq0atUqFRcXa+rUqaqpqWn26+Tl5SkuLi60DBo0yO2QAACdjOs3iCxevFiHDh3SH/7wh7D1n33/Q3p6ukaPHq2UlBRt375dWVlZTb5OTk6OsrOzQ7eDwSBFBADdhKsSWrJkiV577TXt2bNHAwcOvOa2SUlJSklJUUlJSbP3e71eeb1eN8MAAHRyRiXkOI6WLFmi3/3udyosLFRqauoXZsrLy1VWVubq3f8AgK7N6DmhRYsW6cUXX9SmTZvk8/kUCAQUCAR06dIlSZenPHnyySe1b98+nThxQoWFhZo5c6b69eun2bNnt8kDAAB0XkZXQmvXrpUkTZ48OWz9unXrtGDBAkVEROjw4cPauHGjLly4oKSkJE2ZMkVbtmyRz+drtUEDALoG4z/HXUtMTIx27NhxXQMCAHQfHueLmqWdBYNBxcXF2R5Gp+VmRuzmXrXYEvv27TPOPPLII8aZc+fOGWd69uxpnJGkxx9/3Djz0EMPGWd27txpnEHnkJCQYJzp27evcSYQCBhnpP9/f2d7qKioUJ8+fa65DROYAgCsoYQAANZQQgAAayghAIA1lBAAwBpKCABgDSUEALCGEgIAWEMJAQCsoYQAANZQQgAAayghAIA1TGAKAGgTTGAKAOjQKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAmg5XQh1sKjsAgEst+X3e4UqosrLS9hAAAK2gJb/PO9ws2o2Njfr444/l8/nk8XjC7gsGgxo0aJDKysq+cGbWrozjcBnH4TKOw2Uch8s6wnFwHEeVlZVKTk5Wjx7XvtaJbKcxtViPHj00cODAa27Tp0+fbn2SXcFxuIzjcBnH4TKOw2W2j0NLP5Knw/05DgDQfVBCAABrOlUJeb1eLV++XF6v1/ZQrOI4XMZxuIzjcBnH4bLOdhw63AsTAADdR6e6EgIAdC2UEADAGkoIAGANJQQAsIYSAgBY06lK6Je//KVSU1PVs2dPjRo1Su+8847tIbWr3NxceTyesMXv99seVpvbs2ePZs6cqeTkZHk8Hm3bti3sfsdxlJubq+TkZMXExGjy5Mk6cuSIncG2oS86DgsWLGhyfowbN87OYNtIXl6exowZI5/Pp4SEBM2aNUvHjh0L26Y7nA8tOQ6d5XzoNCW0ZcsWLV26VMuWLdPBgwd15513KjMzUydPnrQ9tHb1ta99TadPnw4thw8ftj2kNldVVaURI0ZozZo1zd6/cuVKrV69WmvWrFFxcbH8fr+mTZvW5SbD/aLjIEnTp08POz/efPPNdhxh2ysqKtKiRYu0f/9+5efnq76+XhkZGaqqqgpt0x3Oh5YcB6mTnA9OJ3Hbbbc53/ve98LWfeUrX3GefvppSyNqf8uXL3dGjBhhexhWSXJ+97vfhW43NjY6fr/fee6550Lrqqurnbi4OOff//3fLYywfXz+ODiO48yfP9+55557rIzHljNnzjiSnKKiIsdxuu/58Pnj4Did53zoFFdCtbW1OnDggDIyMsLWZ2RkaO/evZZGZUdJSYmSk5OVmpqq+++/X8ePH7c9JKtKS0sVCATCzg2v16tJkyZ1u3NDkgoLC5WQkKBhw4bpkUce0ZkzZ2wPqU1VVFRIkuLj4yV13/Ph88fhis5wPnSKEjp79qwaGhqUmJgYtj4xMVGBQMDSqNrf2LFjtXHjRu3YsUO//vWvFQgENGHCBJWXl9semjVXvv/d/dyQpMzMTL300ksqKCjQqlWrVFxcrKlTp6qmpsb20NqE4zjKzs7WHXfcofT0dEnd83xo7jhIned86HAf5XAtn/98IcdxmqzryjIzM0P/Hj58uMaPH68vf/nL2rBhg7Kzsy2OzL7ufm5I0ty5c0P/Tk9P1+jRo5WSkqLt27crKyvL4sjaxuLFi3Xo0CH94Q9/aHJfdzofrnYcOsv50CmuhPr166eIiIgm/5M5c+ZMk//xdCexsbEaPny4SkpKbA/FmiuvDuTcaCopKUkpKSld8vxYsmSJXnvtNe3evTvs88e62/lwtePQnI56PnSKEoqOjtaoUaOUn58ftj4/P18TJkywNCr7ampqdPToUSUlJdkeijWpqany+/1h50Ztba2Kioq69bkhSeXl5SorK+tS54fjOFq8eLG2bt2qgoICpaamht3fXc6HLzoOzemw54PFF0UY2bx5sxMVFeW88MILzgcffOAsXbrUiY2NdU6cOGF7aO3miSeecAoLC53jx487+/fvd+6++27H5/N1+WNQWVnpHDx40Dl48KAjyVm9erVz8OBB569//avjOI7z3HPPOXFxcc7WrVudw4cPOw888ICTlJTkBINByyNvXdc6DpWVlc4TTzzh7N271yktLXV2797tjB8/3hkwYECXOg7/+I//6MTFxTmFhYXO6dOnQ8unn34a2qY7nA9fdBw60/nQaUrIcRznF7/4hZOSkuJER0c7t956a9jLEbuDuXPnOklJSU5UVJSTnJzsZGVlOUeOHLE9rDa3e/duR1KTZf78+Y7jXH5Z7vLlyx2/3+94vV5n4sSJzuHDh+0Oug1c6zh8+umnTkZGhtO/f38nKirKGTx4sDN//nzn5MmTtofdqpp7/JKcdevWhbbpDufDFx2HznQ+8HlCAABrOsVzQgCArokSAgBYQwkBAKyhhAAA1lBCAABrKCEAgDWUEADAGkoIAGANJQQAsIYSAgBYQwkBAKz5P4RKjLP+986wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Ankle Boot\n"
     ]
    }
   ],
   "source": [
    "# get the names for each label\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "# Display a sample image and label\n",
    "print(f'We shall visualize a sample image. Notice that the batches reshuffle after\\\n",
    "all batches have been iterated through. \\n')\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f'Feature batch shape: {train_features.size()}')\n",
    "print(f'Label batch shape: {train_labels.size()}')\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0].item()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Random Image')\n",
    "plt.show()\n",
    "print(f'Label: {labels_map[label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e4f76-5560-42cb-80c4-a639e5ab9864",
   "metadata": {},
   "source": [
    "As we have seen, we can index `Datasets` much like we access an item in an array: `training_data[index]`. We shall visualize more of our dataset using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1299e67-e609-42e0-b010-51446fae2769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8,8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item() # get a random integer as an index. .item() method extracts the value of a single entry\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(cols, rows, i) # add subplots\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap='gray') # .squeeze() method removes any dimensions of size 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6f81c-07a5-4df2-8ba4-621c06f65fa2",
   "metadata": {},
   "source": [
    "## Transforming (Data Preprocessing)\n",
    "\n",
    "We want to train a neural network for classifying the FashionMNIST images. But before we do that, we want to make some minor adjustments to our data so that it will be in a form acceptable for training. This doesn't change the information that we have in the data, but is necessary for us to be able to pass the data into a model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861a9d14-61f5-4134-bcae-0fcb5fb1a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    target_transform = Lambda(lambda y:\n",
    "                              torch.zeros(10, dtype = torch.float).scatter_(0, torch.tensor(y), value = 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e09134-b014-488f-ba89-d2d367afb0b7",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now we are ready to build our nueral network model. In PyTorch, every neural network subclasses from torch.nn.Module, and is required to have `__init__` and `forward`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9add21-f052-4b61-8716-81ae04d603c3",
   "metadata": {},
   "source": [
    "But before we do that, it helps to have a good hardware to speed up our training time. The following selects a fancy hardware so that, in case we do have any at hand, we can use them. If you don't know what MPS or GPU is, don't worry about what the following code is doing. It is simply picking a good device at our hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31694533-139e-4ce9-94d9-6600f3af7f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available()\n",
    "          else\n",
    "          'mps' if torch.backends.mps.is_available()\n",
    "          else\n",
    "          'cpu'\n",
    "         )\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a935cf6-e5e0-485f-bfcf-37b6f262f395",
   "metadata": {},
   "source": [
    "Now we build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1735d147-445f-486a-9ce0-003b7d6c2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "                    nn.Linear(28*28, 512), # The expected input for our neural network is 28*28 = 784\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65946fa5-a0b4-4407-9f97-a395bbb6edb1",
   "metadata": {},
   "source": [
    "The neural network we created is a fully connected neural network that has the dimension of the input vector as `28*28 = 784`. Its first hidden layer has the output vector dimension as 512, applies the ReLU function. The output vector from the first layer gets passed onto the second layer, whose output vector has dimension 512 also. We apply the ReLU function again to the output of the second hidden layer, and the third layer takes that and outputs a vector of dimension 10. When we input a 28 by 28 image, it first gets flattened by `nn.Flatten()` to be transformed into a vector of size 784, and then gets passed through the layers.\n",
    "\n",
    "We now create an instance of `NeuralNetwork` and move it to the `device` that we have, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f3d8d4-b676-4208-bcd4-9c5bd2d2b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295d45e-bdf5-42b3-87da-429b9e781f66",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This will execute the mode's `forward` method, along with some background operations. We do not call `model.forward()` directly.\n",
    "\n",
    "Calling the model on an input returns a 2D tensor with rows as individual images and columns as individual raw predicted values. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078f8d9f-e615-4137-bdf1-d444b311f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0908,  0.0070, -0.1127, -0.1024,  0.1180,  0.0823,  0.0275,  0.0201,\n",
      "          0.0221,  0.0119],\n",
      "        [-0.0589,  0.0512, -0.0722, -0.1110,  0.0510,  0.0715,  0.0578,  0.0540,\n",
      "          0.0155,  0.0237],\n",
      "        [-0.0393,  0.0132, -0.0749, -0.0476,  0.0681,  0.0777,  0.0447,  0.0133,\n",
      "          0.0081,  0.0680]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac362c9-2900-4200-84b1-07d75daf8fa6",
   "metadata": {},
   "source": [
    "As you can see, the fianl output is not a probability, since if they were, the second row (for example) would add up to 1, which is not the case (each entry in the 10 columns in the second rows are mostly less than 0.1). In theory, the output of the `nn.Linear(512,10)` layer can be any real number, but since we have pre-applied the `ToTensor` method as `target_transform` in the training set, the raw output `logits` above will typically have small values.\n",
    "\n",
    "We now pass the prediction through an instance of the `nn.Softmax` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f1161a9-66b5-4dbf-8c53-35e24975caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0912, 0.1006, 0.0893, 0.0902, 0.1124, 0.1085, 0.1027, 0.1019, 0.1021,\n",
      "         0.1011],\n",
      "        [0.0933, 0.1042, 0.0921, 0.0886, 0.1042, 0.1063, 0.1049, 0.1045, 0.1005,\n",
      "         0.1014],\n",
      "        [0.0948, 0.0999, 0.0915, 0.0940, 0.1055, 0.1065, 0.1031, 0.0999, 0.0994,\n",
      "         0.1055]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1) # dim=1 means that the softmax function will be applied along the second dimension, so that the sum of entries in each row will be 1\n",
    "pred_probs = softmax(logits)\n",
    "print(pred_probs)\n",
    "y_pred = pred_probs.argmax(1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97c900-2897-4d92-bb71-1c895a45fb9a",
   "metadata": {},
   "source": [
    "### Optional: What each layers are doing\n",
    "\n",
    "We take a closer look at what each layers are doing. You can also look these up in the PyTorch website, but we here give a brief demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469a7e84-f37c-4914-b870-3f1b1583c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original input size is: torch.Size([3, 28, 28])\n",
      "The size of the tensor after applying nn.Flatten() is: torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# sample tensor we'll be using\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(f'The original input size is: {input_image.size()}')\n",
    "\n",
    "# nn.Flatten\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(f'The size of the tensor after applying nn.Flatten() is: {flat_image.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ec935-9570-4703-a62b-05e5224a2cdc",
   "metadata": {},
   "source": [
    "Notice that `nn.Flatten(input_image)` would give us an error. Instead, it's `nn.Flatten()(input_image)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71940e3e-a512-4326-8d14-17e660aaafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor after applying nn.Linear is: torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear. The following is the same as nn.Linear(28*28, 20)\n",
    "layer1 = nn.Linear(in_features = 28*28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(f'Size of the tensor after applying nn.Linear is: {hidden1.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3db873-1882-481a-bfec-ae53e786a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1870, -0.9514,  0.1243, -0.3497, -0.3728, -0.3635,  0.2170,  0.1377,\n",
      "         -0.3104,  0.3393, -0.0541, -0.0610,  0.1358, -0.2247,  0.0365,  0.2554,\n",
      "         -0.3283,  0.3719, -0.0969, -0.3449],\n",
      "        [ 0.0407, -0.5282,  0.5733, -0.6004, -0.5366, -0.1322,  0.2875, -0.0194,\n",
      "         -0.3333,  0.3306,  0.0414, -0.3533,  0.1580, -0.3714,  0.1039,  0.5302,\n",
      "         -0.1556,  0.6729,  0.1281,  0.1704],\n",
      "        [-0.1360, -0.3615, -0.0258, -0.0758, -0.6151, -0.3228,  0.6404,  0.0452,\n",
      "         -0.6077,  0.2047, -0.2270, -0.4152,  0.1567, -0.2534, -0.1479,  0.6098,\n",
      "         -0.0227,  0.3462,  0.0735,  0.0626]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1870, 0.0000, 0.1243, 0.0000, 0.0000, 0.0000, 0.2170, 0.1377, 0.0000,\n",
      "         0.3393, 0.0000, 0.0000, 0.1358, 0.0000, 0.0365, 0.2554, 0.0000, 0.3719,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0407, 0.0000, 0.5733, 0.0000, 0.0000, 0.0000, 0.2875, 0.0000, 0.0000,\n",
      "         0.3306, 0.0414, 0.0000, 0.1580, 0.0000, 0.1039, 0.5302, 0.0000, 0.6729,\n",
      "         0.1281, 0.1704],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6404, 0.0452, 0.0000,\n",
      "         0.2047, 0.0000, 0.0000, 0.1567, 0.0000, 0.0000, 0.6098, 0.0000, 0.3462,\n",
      "         0.0735, 0.0626]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.ReLU()\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e36b3-f658-4684-8476-2f186201640a",
   "metadata": {},
   "source": [
    "`nn.Sequential` is an ordered container of modules. The data is passed through all the modules in the same order as defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e195df9c-909b-4111-8946-604fb3a5228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1708, -0.0255,  0.0236,  0.0456, -0.1212,  0.3107, -0.4031,  0.3220,\n",
      "          0.0032, -0.2503],\n",
      "        [ 0.1525, -0.0521,  0.0477,  0.1656, -0.2837,  0.2982, -0.3829,  0.3728,\n",
      "          0.0026, -0.2522],\n",
      "        [ 0.1415, -0.0459,  0.1343,  0.0582, -0.0784,  0.2500, -0.4585,  0.3217,\n",
      "          0.0725, -0.2251]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3,28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f96c05-2924-4a03-8b04-22a8026be831",
   "metadata": {},
   "source": [
    "According to PyTorch guide: \"The last linear layer of the neural network returns logits - raw values in $[-\\infty, +\\infty]$ - which are passed to the nn.Softmax module. The logits are scaled to values $[0,1]$ representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31aba315-71e0-48f8-87ad-380bc794eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb73c3-bc2b-4518-a3ea-2f1a7a51d1fe",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "\n",
    "If you know the theory behind a neural network, you know that there are parameters involved for each layer of our neural network. Subclassing `nn.Module` automatically tracks all field defined inside our model object and we can access all parameters using `parameters()` or `named_parameters()` methods.\n",
    "\n",
    "We shall interate over each parameter and print its size and its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380d5dec-f7f7-4ca4-9418-31c241f55420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0029, -0.0161,  0.0186,  ..., -0.0090,  0.0330,  0.0091],\n",
      "        [ 0.0311, -0.0067,  0.0135,  ...,  0.0307,  0.0290, -0.0293]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([0.0061, 0.0223], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0240, -0.0327, -0.0384,  ...,  0.0054, -0.0282, -0.0338],\n",
      "        [ 0.0440,  0.0285, -0.0212,  ..., -0.0150, -0.0198, -0.0066]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0398, -0.0383], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[-0.0034, -0.0121, -0.0324,  ...,  0.0378, -0.0364, -0.0273],\n",
      "        [-0.0087,  0.0181,  0.0002,  ...,  0.0176, -0.0280, -0.0386]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0244,  0.0186], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Model structure: {model}\\n\\n')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c8292-483a-40db-8640-b80c94c05976",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In order to do gradient descent, we implement the backpropagation algorithm which is just chain rule in calculating the derivative. To compute the gradients, we use the PyTorch's built-in differentiation engine called `torch.autograd`. We first take a look at a simple case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4571ae2f-615d-4d0e-9a48-10fac6e66f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad = True)\n",
    "b = torch.randn(3, requires_grad = True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba3359-80e7-4c1b-85ac-d2ee1fdaef05",
   "metadata": {},
   "source": [
    "Here, the `requires_grad = True` tells PyTorch that we are going to differentiate the loss function with respect to those variables.\n",
    "\n",
    "According to PyTorch guide, \"a function that we apply to tensors to construct computational graph is in fact an object of class `Function`. This object knows how to compute the function in the *forward* direction ... as well as the *backward* direction, e.g. backpropagation. A reference to the backward propagation function is stored in `grad_fn` property of a tensor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a240a7-9227-421a-baed-585d53246559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000019552FF53F0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000019552FF5390>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efeffd-b37b-4078-8ba5-2fff7ed373bf",
   "metadata": {},
   "source": [
    "Now we computer the derivative with respect to the variables for which we have `requres_grad = True`, i.e. we compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ where $L$ is `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6af359da-4816-4ca8-bff2-a88856c8fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2803, 0.0461, 0.0003],\n",
      "        [0.2803, 0.0461, 0.0003],\n",
      "        [0.2803, 0.0461, 0.0003],\n",
      "        [0.2803, 0.0461, 0.0003],\n",
      "        [0.2803, 0.0461, 0.0003]])\n",
      "tensor([0.2803, 0.0461, 0.0003])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11627213-da27-4b77-85f5-1defa740b318",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking\n",
    "\n",
    "From PyTorch documentation: \"by default, all tensors with `requires_grad = True` are tracking their computational history and support gradient computation.\" However, there are cases when we don't want to track that, e.g. when we have already trained the model and we just want to run a vector through the forward loop. Another situation when we don't want to track the gradients is when we want to make some parameters in a neural network as *forzen parameters*, e.g. when we are doing transfer learning. We also speed up computations when we are doing forward pass without gradient tracking. \n",
    "\n",
    "we can stop tracking by writing `with torch.no_grad(): ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a6d5566-817b-4b9d-b4a6-b9bb3b938ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c9f59-4365-4a93-b7bc-d1c8c45956bf",
   "metadata": {},
   "source": [
    "We can also write `z.detach()` to do the same thing, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa63eb05-1d8c-40da-95d8-eb2046c086e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c12a6a-54bf-435b-9768-d0492c761fea",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters\n",
    "\n",
    "We are now ready to train our data, using a selected optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e640fa-70c6-4f49-85ea-16684f2ba481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4446efe-b371-483e-b44b-8dc2dc591c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1e435-1955-4bdd-a569-4a008321ae04",
   "metadata": {},
   "source": [
    "We pick an optimizer in `torch.optim`. we then traing the model. According to PyTorch guide:\n",
    "\n",
    "\"Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "- Call `optimizer.zero_grad()` to reset the gradeitns of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "- Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss with respect to each parameter.\n",
    "- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\"\n",
    "\n",
    "Below is the full implementation. We frist define a method `train_loop` that loops over our optimization code, and another moethod `test_loop` that evaluates how well our model is performing against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1cad2e7-05ee-41d5-b5a6-3c689527adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # number of training examples\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"size: {size}\")\n",
    "    # Set the model to training mode - this is important for batch normalization and dropout layers\n",
    "    # unncessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch_num, (X, y) in enumerate(dataloader):\n",
    "        # make a prediction and compute the loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # For every 100 batches, print the loss and the number of total data that's been processed so far after the end of the batch\n",
    "        if batch_num % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            current = batch_num*batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f} [ Processed {current:>5d} images out of the total {size:>5d} images ]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # set the model to evaluation mode - this is important for batch normalization and dropout layers\n",
    "    # unncessary in this situation vbut added for best practices\n",
    "    model.eval()\n",
    "    # size of the test examples\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # initialize the test loss function value and the number of correct guesses\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # evaluate the model with torch.no_rad() is important since it ensures that no gradients are computed during test mode\n",
    "    # also reduces unnecessary gradient computations and memory usages\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # make predictions for the current batch\n",
    "            pred = model(X)\n",
    "            # add the test loss for our current batch to test_loss; we will take an average of the test loss later\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # add the number of correct arguments to the variable `correct`.\n",
    "            # more specifically, pred.argmax(1) returns the column number (from 0 to 9) of the max number in each row so that the output dim is 1,\n",
    "            # checks for each entry of the pred.argmax(1) whether that's the correct prediction, getting us a tensor with True or False entry.\n",
    "            # we then convert that into a numeric (torch.float), so that `True` entries become 1 and `False` entries become 0.\n",
    "            # we then sum all the 1s, getting us a tensor with a single entry that effectively captures the number of correct predictions.\n",
    "            # finally, we get the item in that tensor, and add that value to `correct` variable.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # compute the average test loss and accuracy. \n",
    "    # Notice that test_loss is divided by the total number of batches and not by the total number of test examples, since each loss_fn(pred, y) that we added to test_loss already is an average of a single batch\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52478a97-f197-42b7-8f47-852947955544",
   "metadata": {},
   "source": [
    "We now initialize the loss function and optimizer, then passing it to `train_loop` and `test_loop` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50d9fcd0-c95c-4d0c-812a-2970ff64b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 2.319693 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 2.293995 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 2.285071 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 2.274010 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 2.250558 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 2.229628 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 2.226355 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 2.210713 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 2.187650 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 2.185826 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 2.1607079490734513 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 2.167321 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 2.135589 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 2.144984 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 2.096795 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 2.066370 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 2.025127 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 2.035183 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 1.976346 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 1.899901 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 1.926609 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.8831367128214258 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 1.946610 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 1.874723 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 1.818777 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 1.797590 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 1.721196 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 1.659650 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 1.596637 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 1.572081 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 1.607400 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 1.512642 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.5078504450002295 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 1.508465 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 1.484230 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 1.376500 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 1.401843 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 1.303513 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 1.327083 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 1.314988 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 1.336581 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 1.330649 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 1.217700 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.2445351363746984 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 1.359275 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 1.197914 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 1.217592 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 1.286302 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 1.159992 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 1.259505 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 1.142705 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 1.129100 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 1.039771 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.993246 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.0860004447827674 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 1.025971 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 1.236204 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 1.075777 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 1.085288 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 1.061552 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 1.103843 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 1.115503 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 1.149803 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.993181 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.872800 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.9815110184584454 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.835560 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.982928 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 1.055324 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.931786 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 1.060060 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.952717 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 1.034092 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.874441 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.918182 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 1.010143 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.9108020640482568 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.922089 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.937514 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.826370 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.850419 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.883762 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.803804 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.811496 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.952899 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.793982 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.848798 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.8600409425747623 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.841380 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.806460 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.762051 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.869032 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.938718 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.779773 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.751996 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.694399 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.694146 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.823394 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.8215183346134842 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.884589 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.824510 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.715605 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.899539 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.716007 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.828830 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.721362 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.745359 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.716683 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.762562 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.789542977217656 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.750916 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 1.044948 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.733018 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.822487 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.727645 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.704037 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.732153 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.924712 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.751219 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.738482 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.7643976215344326 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.843562 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.831555 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.669502 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.755084 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.691966 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.689085 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.633606 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.863694 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.637347 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.762239 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.7443067446635787 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.621487 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.740587 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.718449 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.881947 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.691277 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.775250 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.740826 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.649848 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.676686 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.854865 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.7218261397188637 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.810080 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.606359 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.714389 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.766571 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.529309 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.610787 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.592257 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.583278 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.794793 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.796332 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.7061112697716732 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------------\n",
      "size: 60000\n",
      "loss: 0.638444 [ Processed    64 images out of the total 60000 images ]\n",
      "loss: 0.693361 [ Processed  6464 images out of the total 60000 images ]\n",
      "loss: 0.793200 [ Processed 12864 images out of the total 60000 images ]\n",
      "loss: 0.777358 [ Processed 19264 images out of the total 60000 images ]\n",
      "loss: 0.653279 [ Processed 25664 images out of the total 60000 images ]\n",
      "loss: 0.683072 [ Processed 32064 images out of the total 60000 images ]\n",
      "loss: 0.582169 [ Processed 38464 images out of the total 60000 images ]\n",
      "loss: 0.601734 [ Processed 44864 images out of the total 60000 images ]\n",
      "loss: 0.570836 [ Processed 51264 images out of the total 60000 images ]\n",
      "loss: 0.713585 [ Processed 57664 images out of the total 60000 images ]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.6878304037318868 \n",
      "\n",
      "We had 15 epochs. Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(f\"We had {epochs} epochs. Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75c343-38bf-4494-b308-54229f13f808",
   "metadata": {},
   "source": [
    "Notice that with a CPU it takes quite some time to run the above. We can see a benefit of having using the Stochastic Gradient Descent, `torch.optim.SGD`.\n",
    "\n",
    "Notice also that the test accuracy increases for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361415c-21dd-4911-aa6c-29beb2649aae",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "We now save our model. the learned parameters is internally stored in an internal state dictionary called `state_dict`.\n",
    "\n",
    "A `.pth` file is a common file extension used in PyTorch to save model weights or entire models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c03343f-5793-4632-b33a-d5a2dfa3304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# save the model parameters by called torch.save\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4775435-ed19-4dcb-8ea2-d5c468c684f4",
   "metadata": {},
   "source": [
    "To load a model weight, we first create an instance of the same model first, and then load the parameters using `load_state_dict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fb6931d-1d0f-44d6-84f6-2fc09f6e6be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srhee\\AppData\\Local\\Temp\\ipykernel_2508\\3151015651.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  another_model.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the same model structure. Notice that this model has untrained parameters\n",
    "another_model = NeuralNetwork().to(device)\n",
    "# loading the trained weights to our another_model\n",
    "another_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# for consistent inference results\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af919c4c-7983-44c4-920b-e376d21bd44e",
   "metadata": {},
   "source": [
    "According to PyTorch guide: \"be sure to call `model.eval()` method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e349c7-4bcf-4f0a-8596-f6095405d2da",
   "metadata": {},
   "source": [
    "If we want to save not only the weights but also the model structure, we can pass the model directly to `torch.save` rather than the `model.state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63f60ba8-98bc-443d-bdf9-fa23006ca0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srhee\\AppData\\Local\\Temp\\ipykernel_2508\\3616403566.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  yet_another_model = torch.load('model.pth')\n"
     ]
    }
   ],
   "source": [
    "# save the model structure *and* the model parameter\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# if we want to load the model, we can do like this\n",
    "yet_another_model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0666976-7973-4a49-8c5f-259cb2793b2f",
   "metadata": {},
   "source": [
    "Thank you for reading! All the above *very closely* follows the [PyTorch Learn the Basics guide](https://pytorch.org/tutorials/beginner/basics/intro.html). More information about this can be found in the link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
