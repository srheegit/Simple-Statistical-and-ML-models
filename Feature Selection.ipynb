{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c25c0f-d6f9-4ae8-bda3-a6c0e88e0651",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Author: Sahngyoon Rhee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bbb78-f989-489d-b970-f661f299fc09",
   "metadata": {},
   "source": [
    "Feature selection is, as the name suggests, a process of selecting a subset of all features so that only the more relevant features will be involved in makign a machine learning model. This can be useful for more reason than making the training more efficient. When we have a feature that we know is uncorrelated to the target variable, it makes sense to remove that feature.\n",
    "\n",
    "For example, suppose that we are creating a machine learning model for a company that predicts the sale of various products for the next quarter based on the data of all previous transactions from its customers. The various features of the transaction could be customer age, name of the product sold, the number of units sold, date of transaction, sales amount, monthly-billing or lump payment, and weather of the date of transaction. While weather can certainly be a factor in a sale of a physical commodity (e.g. an umbrella or an instant noodle), it certainly won't be helpful - and indeed irrelevant - in predicting the sale of a software product from the company.\n",
    "\n",
    "Feature selection can also be used to remove redundant information from the set of feature variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc117ee-ad5c-42ab-943f-fb8f37c915ba",
   "metadata": {},
   "source": [
    "## Various methods for Feature Selection\n",
    "\n",
    "There are various methods for feature selection. They can be supervised or unsupervised. \n",
    "\n",
    "The unsupervised feature selection methods do not use the target variable, and hence the reason why it's called unsupervised. A prime example of unsupervised feature selection is Principal Component Analysis, or PCA, which seeks to project a set of given points to a lower-dimensional hyperplane, effectively reducing the number of features.\n",
    "\n",
    "Supoervised feature selection methods can be divided up into the following three:\n",
    "\n",
    "- Filter methods: These methods evaluate the relevance of each feature independently of the model, by looking at measures such as correlation coefficients.\n",
    "- Wrapper methods: These methods evaluate feature subsets based on model performance. Two examples are forward and backward feature selection.\n",
    "- Embedded methods: These methods perform feature selection during the model training process. Random Forest is an example of this.\n",
    "\n",
    "Next, we shall look at implementation of one method from each of the three class of supervised feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd14d9-62a7-4d69-92e5-05d4bb7e477d",
   "metadata": {},
   "source": [
    "### Filter methods\n",
    "\n",
    "We shall look at how we can use the correlation coefficient to filter out less relevant features. We will use the `breast_cancer` dataset from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49b2735-7bac-4d35-b56a-d942994a0fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'radius error', 'perimeter error', 'area error', 'compactness error', 'concavity error', 'concave points error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
      "Original number of features: 30\n",
      "Selected number of features: 25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# create a pandas dataframe\n",
    "df = pd.DataFrame(X, columns=cancer.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Get the correlation of each feature with the target\n",
    "target_corr = corr_matrix['target'].drop('target')\n",
    "\n",
    "# Set a threshold for correlation (e.g., 0.2)\n",
    "threshold = 0.2\n",
    "\n",
    "# Select features with correlation above the threshold\n",
    "selected_features = target_corr[abs(target_corr) > threshold].index.tolist()\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "X_selected = df[selected_features]\n",
    "\n",
    "print(\"Original number of features:\", X.shape[1])\n",
    "print(\"Selected number of features:\", X_selected.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f6f9e-d85e-4a26-9cc5-02118bd0e983",
   "metadata": {},
   "source": [
    "We notice that this method of feature selection is independent of a choice of a model; indeed, we didn't even create a machine learning model in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582a48c-bd7e-4095-88cb-697a02a2c41f",
   "metadata": {},
   "source": [
    "### Wrapper Methods\n",
    "\n",
    "In contrast to filter methods, which is independent of a choice of model, wrapper methods evaluate feature subsets based on model performance. We shall take a look at the forward feature selection, which starts out with an empty set of features and adds one feature at a time based on a specifiied criterion (such as the model performance).\n",
    "\n",
    "Backward feature selection works a similar way, except that we start from a complete set of features in the beginning and *remove* a feature at a time based on a specific criterion.\n",
    "\n",
    "Here, we take a look at an example using the `breast_cancer` dataset from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9312a5-4976-45b8-82c7-b52534f92499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['worst concave points', 'worst radius', 'worst texture']\n",
      "Best cross-validation score: 0.9613569321533924\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Forward feature selection\n",
    "selected_features = []\n",
    "remaining_features = list(feature_names)\n",
    "best_score = 0\n",
    "\n",
    "while remaining_features:\n",
    "    scores = []\n",
    "    for feature in remaining_features:\n",
    "        current_features = selected_features + [feature]\n",
    "        X_subset = df[current_features]\n",
    "        score = cross_val_score(clf, X_subset, y, cv=5).mean()\n",
    "        scores.append((score, feature))\n",
    "    \n",
    "    # Find the best new feature\n",
    "    best_new_score, best_new_feature = max(scores)\n",
    "    \n",
    "    if best_new_score > best_score:\n",
    "        best_score = best_new_score\n",
    "        selected_features.append(best_new_feature)\n",
    "        remaining_features.remove(best_new_feature)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Best cross-validation score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e012b-d8af-4ff0-b6a0-ce206c95b636",
   "metadata": {},
   "source": [
    "### Embedded methods\n",
    "\n",
    "Embedded methods perform feature selection while a chosen machine learning model is training. Random Forest is an example of an embedded method since it can provide feature importance scores as part of the model training.\n",
    "\n",
    "Here is an example using the same dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b4afa3-31e8-41c2-9d46-450b7c8eac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Importance\n",
      "23               worst area    0.139357\n",
      "27     worst concave points    0.132225\n",
      "7       mean concave points    0.107046\n",
      "20             worst radius    0.082848\n",
      "22          worst perimeter    0.080850\n",
      "2            mean perimeter    0.067990\n",
      "6            mean concavity    0.066917\n",
      "3                 mean area    0.060462\n",
      "26          worst concavity    0.037339\n",
      "0               mean radius    0.034843\n",
      "13               area error    0.029553\n",
      "25        worst compactness    0.019864\n",
      "21            worst texture    0.017485\n",
      "1              mean texture    0.015225\n",
      "10             radius error    0.014264\n",
      "24         worst smoothness    0.012232\n",
      "5          mean compactness    0.011597\n",
      "12          perimeter error    0.010085\n",
      "28           worst symmetry    0.008179\n",
      "4           mean smoothness    0.007958\n",
      "19  fractal dimension error    0.005942\n",
      "16          concavity error    0.005820\n",
      "15        compactness error    0.005612\n",
      "14         smoothness error    0.004722\n",
      "29  worst fractal dimension    0.004497\n",
      "17     concave points error    0.003760\n",
      "11            texture error    0.003744\n",
      "18           symmetry error    0.003546\n",
      "8             mean symmetry    0.003423\n",
      "9    mean fractal dimension    0.002615\n",
      "Top 10 features: ['worst area', 'worst concave points', 'mean concave points', 'worst radius', 'worst perimeter', 'mean perimeter', 'mean concavity', 'mean area', 'worst concavity', 'mean radius']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Select the top N features (e.g., top 10)\n",
    "top_n_features = feature_importance_df.head(10)['Feature'].tolist()\n",
    "\n",
    "print(\"Top 10 features:\", top_n_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
